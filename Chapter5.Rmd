---
title: "Chapter 5"
output:
  html_document:
    toc: true
    toc_depth: 3
---

```{r include=FALSE}
# Setting Knit options for the document
knitr::opts_chunk$set( message=FALSE, warning=FALSE)
```
\newcommand{\indep}{\perp \!\!\! \perp}


# 5.1: Supurious Correlations
## We observe a puzzle: Marriage rates and divorce rates are positively correlated
```{r}
library(rethinking)
data(WaffleDivorce)
d <- WaffleDivorce

d$W <- d$WaffleHouses
d$A <- scale(d$MedianAgeMarriage)
d$D <- scale(d$Divorce)
d$M <- scale(d$Marriage)
```

Let's fit a bayesian model to the three variables -- Waffle Houses, Age at marriage and Marriage Rate.

```{r}
waffle_divorce <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bW*W,
    bW ~ dnorm(0, 0.2),
    a ~ dnorm(0, 0.2),
    sigma ~ dexp(1)
  ),
  data = d
)
age_divorce <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bA*A,
    bA ~ dnorm(0, 0.2),
    a ~ dnorm(0, 0.2),
    sigma ~ dexp(1)
  ),
  data = d
)

marriage_divorce <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bM*M,
    bM ~ dnorm(0, 0.2),
    a ~ dnorm(0, 0.2),
    sigma ~ dexp(1)
  ),
  data = d
)
```

```{r}
waffle_divorce_posterior <- MASS::mvrnorm( n=1e3 , mu=coef(waffle_divorce) , Sigma=vcov(waffle_divorce))
waffle_seq <- seq(from=min(d$W)-1, to = max(d$W) + 1, length.out = 30)
input_matrix <- rbind(rep(1,length(waffle_seq)), waffle_seq)
mean_divorce_sim_m5_1 <- waffle_divorce_posterior[,c("a","bW")] %*% input_matrix
mean_divorce_PI_m5_1 <- apply(mean_divorce_sim_m5_1, 2, rethinking::PI, prob=0.89)

age_divorce_posterior <- MASS::mvrnorm( n=1e3 , mu=coef(age_divorce) , Sigma=vcov(age_divorce))
age_seq <- seq(from=min(d$A)-1, to = max(d$A) + 1, length.out = 30)
input_matrix <- rbind(rep(1,length(age_seq)), age_seq)
mean_divorce_sim_m5_2 <- age_divorce_posterior[,c("a","bA")] %*% input_matrix
mean_divorce_PI_m5_2 <- apply(mean_divorce_sim_m5_2, 2, rethinking::PI, prob=0.89)

marriage_divorce_posterior <- MASS::mvrnorm( n=1e3 , mu=coef(marriage_divorce) , Sigma=vcov(marriage_divorce))
marriage_seq <- seq(from=min(d$M)-1, to = max(d$M) + 1, length.out = 30)
input_matrix <- rbind(rep(1,length(marriage_seq)), marriage_seq)
mean_divorce_sim_m5_3 <- marriage_divorce_posterior[,c("a","bM")] %*% input_matrix
mean_divorce_PI_m5_3 <- apply(mean_divorce_sim_m5_3, 2, rethinking::PI, prob=0.89)

# Plot the graphs
par(mfcol=c(1,3))

plot(d$WaffleHouses, d$D, main="Waffle Houses and Divorce Rate")
abline(a=mean(waffle_divorce_posterior[,"a"]), b=mean(waffle_divorce_posterior[,"bW"]), lty=2)
shade(mean_divorce_PI_m5_1, waffle_seq)

plot(d$A, d$D, main="Age at marriage and Divorce Rate")
abline(a=mean(age_divorce_posterior[,"a"]), b=mean(age_divorce_posterior[,"bA"]), lty=2)
shade(mean_divorce_PI_m5_2, age_seq)

plot(d$M, d$D, main="Marriage Rate and Divorce Rate")
abline(a=mean(marriage_divorce_posterior[,"a"]), b=mean(marriage_divorce_posterior[,"bM"]), lty=2)
shade(mean_divorce_PI_m5_3, marriage_seq)
```


## Introduce some tools to solve the puzzle: DAGs
A DAG is a way of describing qualitative causal relationships among variables. It isnâ€™t as detailed as a full model description, but it contains information that a purely statistical model does not. Unlike a statistical model, a DAG, if it is correct, will tell you the consequences of intervening to change a variable.

```{r}
library(dagitty)
dag5.1 <- dagitty( "dag {
    A -> D
    A -> M
    M -> D
}")
coordinates(dag5.1) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) )

dag5.1_b <- dagitty( "dag {
    A -> D
    A -> M
}")
coordinates(dag5.1_b) <- list( x=c(A=0,D=1,M=1) , y=c(A=0,D=2,M=-2) )
par(mfcol=c(1,2))
rethinking::drawdag( dag5.1 )
rethinking::drawdag( dag5.1_b)
```

What the DAG on the left says is:
  (1) A directly influences D 
  (2) M directly influences D 
  (3) A directly influences M

The DAG on the right says:
  (1) A directly influences D 
  (2) M does not directly influence D 
  (3) A directly influences M


## Interpreting DAGs: Testable implications of DAGs

Comparing the DAG on the left to the DAG on the right, we can observe that in the right DAG, M influences D only due to the shared influence of A, i.e D and M share a common influence via A. 
If we condition for A, them M and D would be independent. A conclusion we represent as $D \indep M|A$

DAGs are analytical models of reality, not statistical models. This means that there are multiple potential statistical models that can fit the same DAG and it's implications. Here, we will use a linear model to condition for the effect of $A$ on $M$ and $D$. 

We test the two DAGs above using a multi-variate linear model

```{r}
multivar_model <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bA*A + bM*M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data=d
)

rethinking::precis(multivar_model)

# multivar_model_posterior <- MASS::mvrnorm( n=1e3 , mu=coef(multivar_model) , Sigma=vcov(multivar_model))
# waffle_seq <- seq(from=min(d$W)-1, to = max(d$W) + 1, length.out = 30)
# input_matrix <- rbind(rep(1,length(waffle_seq)), waffle_seq)
# mean_divorce_sim_m5_1 <- waffle_divorce_posterior[,c("a","bW")] %*% input_matrix
# mean_divorce_PI_m5_1 <- apply(mean_divorce_sim_m5_1, 2, rethinking::PI, prob=0.89)
```
Note above that we do not simulate the posterior as usual. Why is this? 

This is because, multi-variate distributions are really difficult to simulate. Typical simulation involves generating a matrix of coeffcient values, and then multiplying that matrix by a predictor-value matrix to output the posteriod mean value for the outcome variable, ie

$$ MeanOutcomes = 

\begin{bmatrix}
\mathtt{Coeffienct Samples}
\end{bmatrix}

\begin{bmatrix}
\mathtt{PredictorValues}
\end{bmatrix} $$

The predictor-value matrix is easy to construct when there is only one variable. It's simply 

$$\begin{bmatrix}
\mathbf{1}, \mathtt{PredictorSequence}
\end{bmatrix}$$

where $\mathbf{1}$ is a vector with all 1's, and Predictor Sequence is a sequence of values drawn from the values of the predictor variable, typically via some code like `seq(from=min(Predictor), to=max(Predictor), by = 1)`.

However, when the no of predictors increase, the predictor value matrix becomes significantly larger. If we were to pick $n$ values for each predictor, then the predictor matrix would have $n*n$ rows. If we sample 1000 rows for the posterior coefficients matrix, and have k predictor variables, then our mean outcomes matrix will have 1000*$n^k$ rows. For n = 30 and k = 4, we are left with an 800 mn row matrix. 

## Understanding mulivariate predictions

As described above, generating the matrix of posterior predictions becomes harded as the number of predictors increases. The same applies to analysis which require summarising this matrix. 

Let's say we wish to plot the outcome variable against a predictor variable of interest $X_t$. In the single variate case, we would pick a sequence of $X_t$ values and generate posterior predictions for these values. However, now things have changed. To find the average $Y$ value at a particular $X_t$ value, we would need to account for the other $k-t$ predictors. We're back to the same problem as before, at each of the $n$ values of $X_t$ where we would like to evaluate the outcome, we need to manipulate a matrix with $1000*n^{k-1}$ rows. 

On top of this, we'd also need to consider the covariance structure of the predictor variables. 

This does not mean that generating these outcomes is impossible. We can of course generating them using the vector of coefficients, and the Covaraince matrix. It's just that at this point in the text, we haven't developed the mathematical machinery to do this. As a result we need to look else where to generate predictive plots of outcomes. 

### Partialling out predictor variables

Consider a regression equation of the form $y = \alpha + \Sigma_i \beta_i X_i$ where $\beta_i$ denote the coefficients of the $i$ predictors. We'd like to understand the independent effect of one predictor, say $X_k$. How can we do this?

One way to do this is to partial out the variation in $X_k$ that can be explained by the other predictors. We do this by estimating the following regression: $Mean(X_k) = \alpha' + \Sigma_j\theta_jX_j$
Once we have this regression, the unexplained part of $X_k$ is $X_k - Mean(X_k)$, also known as the _residual_.

```{r}
m5.4 <- quap(
    alist(
        M ~ dnorm( mu , sigma ) ,
        mu <- a + bAM * A ,
        a ~ dnorm( 0 , 0.2 ) ,
        bAM ~ dnorm( 0 , 0.5 ) ,
        sigma ~ dexp( 1 )
) , data = d )

m5.4_posterior <- MASS::mvrnorm( n=1e3 , mu=coef(m5.4) , Sigma=vcov(m5.4))

# Note that instead of generating a sequence of predictor values, we're using the original data itself
m5.4_input_matrix <- rbind(rep(1,length(d$A)),as.vector(d$A))
m5.4_Marriage_Rate_Predict <- m5.4_posterior[,c("a", "bAM")] %*% m5.4_input_matrix
m5.4_Marriage_Rate_mean <- apply(m5.4_Marriage_Rate_Predict, 2, mean)
m5.4_mean_Marriage_CI <- apply(m5.4_Marriage_Rate_Predict, 2, rethinking::PI, prob=0.89)
d$M_resid <- d$M - m5.4_Marriage_Rate_mean
plot(d$D ~ d$M_resid)
```

So, we've residualized the Marriage rate. We can now regress the Divorce rate on the residualized Marriage rate. 


```{r}
m5.4b <- quap(
    alist(
        D ~ dnorm( mu , sigma ) ,
        mu <- a + bM_resid * M_resid ,
        a ~ dnorm( 0 , 0.2 ) ,
        bM_resid ~ dnorm( 0 , 0.25 ) ,
        sigma ~ dexp( 1 )
) , data = d )

m5.4b_posterior <- MASS::mvrnorm( n=1e3 , mu=coef(m5.4b) , Sigma=vcov(m5.4b))

m5.4b_input_matrix <- rbind(rep(1,length(d$M_resid)), as.vector(d$M_resid))
m5.4b_Divorce_Rate_Predict <- m5.4b_posterior[,c("a", "bM_resid")] %*% m5.4b_input_matrix
m5.4b_Divorce_Rate_mean <- apply(m5.4b_Divorce_Rate_Predict, 2, mean)
m5.4b_mean_Divorce_CI <- apply(m5.4b_Divorce_Rate_Predict, 2, rethinking::PI, prob=0.89)
plot(d$D ~ d$M_resid)
abline(a=mean(m5.4b_posterior[,"a"]),
       b=mean(m5.4b_posterior[,"bM_resid"]))
abline(v=0, lty=2)
shade(m5.4b_mean_Divorce_CI,d$M_resid)
```

Well that's unexpected. Not sure what's going on...

Anyway, later, McLerath mentions that we can't do this partialling trick when predictor variables are not related additively. 

### Goodness of Fit plots

The idea here is to plot predicted v/s actually observed values for outcomes to judge the fit. 

We start by fitting the model
```{r}
multivar_model <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bA*A + bM*M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data=d
)

# Extract a bunch of posterior parameter values
multivar_model_posterior <- MASS::mvrnorm( n=1e3 , mu=coef(multivar_model) , Sigma=vcov(multivar_model))
```

Due to the predictor value matrix becoming large, we'll stick to using the original data itself. 

```{r}
# Predictions = Posterior Parameters x Predictor Value matrix
predictor_val_matrix <- rbind(rep(1,length(d$A)), as.vector(d$A), as.vector(d$M))
mu_divorce_predictions <- multivar_model_posterior[,c("a", "bA", "bM")] %*% predictor_val_matrix
mu_divorce_mean <- apply(mu_divorce_predictions, 2, mean)
mu_divorce_PI <- apply(mu_divorce_predictions, 2, rethinking::PI, prob=0.89)
```

```{r}
plot( d$D, mu_divorce_mean, xlab="Observed Divorce", ylab="Predicted Divorce Rates")
abline( a=0 , b=1 , lty=2 )
for ( i in 1:nrow(d) ) lines( rep(d$D[i],2) , mu_divorce_PI[,i] , col=rangi2 ) # Drawss the 89% PI bars
```




### Counterfactual Plots

So the idea here seems to be the following:
  1. Pick a predictor, and a sequence of values of the predictor.
  2. Simulate other predictor variable values at each point in this sequence. How do we do this? We basically estimate all other predictors as a function of our predictor of interest and then evaluate the estimated function at the above sequence of values. 
  
```{r}
library(rethinking)
data(WaffleDivorce)
d <- list()
d$A <- standardize( WaffleDivorce$MedianAgeMarriage )
d$D <- standardize( WaffleDivorce$Divorce )
d$M <- standardize( WaffleDivorce$Marriage )

m5.3_A <- quap(
  alist(
  ## A -> D <- M
    D ~ dnorm(mu, sigma),
    mu <- a + bA*A + bM*M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.2),
    sigma ~ dexp(1),
    
  ## A -> M
    M ~ dnorm(mu_M, sigma_M),
    mu_M <- aM + bAM*A,
    aM ~ dnorm(0, 0.2),
    bAM ~ dnorm(0, 0.2),
    sigma_M ~ dexp(1)
  ), data = d)

A_seq <- seq( from=-2 , to=2 , length.out=30 )
m5.3A_posterior <- MASS::mvrnorm( n=1e3 , mu=coef(m5.3_A) , Sigma=vcov(m5.3_A))

# Simulate M using the estimated function of A
M_sigma <- m5.3A_posterior[,c("sigma_M")]
M_mu <- m5.3A_posterior[,c("aM", "bAM")] %*% rbind(rep(1,length(A_seq)), as.vector(A_seq))
M_sim <- apply(M_mu, 2, function(mu_vector)
                          (rnorm(1e3,mean=mu_vector, sd=M_sigma))
               )

# The matrix M_sim now has 1e3 rows. Each colum contains the simulate Marriage Rate at a particular value of A_seq
# We can use these simulated values of M_sim to simulate D now

D_sigma <- m5.3A_posterior[,c("sigma")]
D_sim <-sapply(
              1:length(A_seq), 
              function(i)
                rnorm(1000, 
                m5.3A_posterior[,c("a", "bA", "bM")] %*% 
                rbind(rep(1,1000), rep(A_seq[i], 1000), as.vector(M_sim[,i])),
              D_sigma)) 


D_sim_mean <- apply(D_sim, 2, mean)
M_sim_mean <- apply(M_sim, 2, mean)

D_sim_PI <- apply(D_sim, 2, rethinking::PI, prob=0.89)
M_sim_PI <- apply(M_sim, 2, rethinking::PI, prob=0.89)

D_simM <-sapply(
              1:length(A_seq), 
              function(i)
                rnorm(1000, 
                m5.3A_posterior[,c("a", "bA", "bM")] %*% 
                rbind(rep(1,1000), rep(0, 1000), as.vector(M_sim[,i])),
              D_sigma)) 


D_sim_meanM <- apply(D_simM, 2, mean)
D_sim_PIM <- apply(D_simM, 2, rethinking::PI, prob=0.89)

par(mfcol=c(1,3))
plot(D_sim_mean ~ A_seq, ylim=c(-2,2.5), xlab="Manipulated A", ylab="Counterfactual Divorce")
shade(D_sim_PI, A_seq)
plot(M_sim_mean ~ A_seq, ylim=c(-2,2.5), xlab="Manipulated A", ylab="Counterfactual Marriage Rate")
shade(M_sim_PI, A_seq)
plot(D_sim_meanM ~ A_seq, ylim=c(-2,2.5), xlab="Manipulated M", ylab="Counterfactual Divorce")
shade(D_sim_PIM, A_seq)

```
  
  
  
# 5.2: Masked Relationship

```{r}
library(rethinking)
data(milk)
d <- na.omit(milk)

# Rescale Variables
d$K <- scale( d$kcal.per.g )
d$N <- scale( d$neocortex.perc )
d$M <- scale( log(d$mass) )
```

```{r}
xseq <- c(-2,2)
N_seq <- seq(from=-2, to=2, length.out = 1e3)

# First set of priors
prior_matrix_1 <- cbind(a=rnorm(1e3, 0, 1), bN=rnorm(1e3, 0, 1), sigma=rexp(1e3, 1))
prior_mu_1 <- prior_matrix_1[,c("a", "bN")] %*% rbind(rep(1, 1e3), as.vector(N_seq))

# Revisedd prioirs
prior_matrix_2 <- cbind(a=rnorm(1e3, 0, 0.2), bN=rnorm(1e3, 0, 0.5), sigma=rexp(1e3, 1))
prior_mu_2 <- prior_matrix_2[,c("a", "bN")] %*% rbind(rep(1, 1e3), as.vector(N_seq))

# Plot the prior plots
par(mfcol=c(1,2))
plot(NULL, xlim=xseq, ylim=xseq, xlab="neocortex percent (std)",
     ylab="kilocal per gram (std)",
     main="a ~ dnorm(0,1) \n bN ~ dnorm(0,1) ")
for ( i in 1:100 ) lines( N_seq , prior_mu_1[i,] , col=col.alpha("black",0.3))

plot(NULL, xlim=xseq, ylim=xseq, xlab="neocortex percent (std)",
        ylab="kilocal per gram (std)",
     main="a ~ dnorm(0, 0.2) \n bN ~ dnorm(0, 0.5)")
for ( i in 1:100 ) lines( N_seq , prior_mu_2[i,] , col=col.alpha("black",0.3))
```

The actual relationship b/w milk energy and neocortex percentage looks something like this:
```{r}
plot(d$K ~ d$N, xlab="neocortex percent (std)",
     ylab="kilocal per gram (std)",
     main="Raw data")
```

```{r}
# We'll use the second set of prioirs to estimate posterioirs

m5.5 <- quap(
  alist(
    K ~ dnorm(mu, sigma),
    mu <- a + bN*N,
    a ~ dnorm(0, 0.2),
    bN ~ dnorm(0, 0.2),
    sigma ~ dexp(1)
  ), data=d
)

m5.5_post_matrix <- MASS::mvrnorm( n=1e3 , mu=coef(m5.5) , Sigma=vcov(m5.5))
```

```{r}
xseq <- seq( from=min(d$N)-0.15 , to=max(d$N)+0.15 , length.out=30 )
mu <- m5.5_post_matrix[,c("a", "bN")] %*% rbind(rep(1,length(xseq)), as.vector(xseq))
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot( K ~ N , data=d, main="Posterior of Milk energy against neocortex %" )
lines( xseq , mu_mean , lwd=2 )
shade( mu_PI , xseq )
```

Now let's plot the posterior of Milk energy against log(body mass)

```{r}
d$logM <- log(d$mass)
d$LM <- scale(d$logM)
```

```{r}
m5.6 <- quap(
  alist(
    K ~ dnorm(mu, sigma),
    mu <- a + bLM*LM,
    a ~ dnorm(0, 0.2),
    bLM ~ dnorm(0, 0.2),
    sigma ~ dexp(1)
  ), data=d
)

m5.6_post_matrix <- MASS::mvrnorm( n=1e3 , mu=coef(m5.6) , Sigma=vcov(m5.6))
```

Plotting the posterior now
```{r}
LM_Seq <- seq( from=min(d$LM)-0.15 , to=max(d$LM)+0.15 , length.out=30 )
m5.6_mu_post <- m5.6_post_matrix[,c("a", "bLM")] %*% rbind(rep(1,length(LM_Seq)), as.vector(LM_Seq))
m5.6_mu_post_mean <- apply(m5.6_mu_post,2,mean)
m5.6_mu_post_PI <- apply(m5.6_mu_post,2,PI)
plot( K ~ LM , data=d, main="Posterior of Milk energy against log of body mass" )
lines(LM_Seq , m5.6_mu_post_mean , lwd=2 )
shade( m5.6_mu_post_PI , LM_Seq )
```

Adding both neocortex % and body mass as predictors

```{r}
m5.7 <- quap(
  alist(
    K ~ dnorm(mu, sigma),
    mu <- a + bN*N + bLM*LM,
    a ~ dnorm(0, 0.2),
    bN ~ dnorm(0, 0.5),
    bLM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

m5.7_post_matrix <- MASS::mvrnorm( n=1e3 , mu=coef(m5.7) , Sigma=vcov(m5.7))
```

```{r}
plot( coeftab( m5.5 , m5.6 , m5.7 ) , pars=c("bLM","bN") )
```

Plotting some counterfactual plots

```{r}
LMseq <- seq( from=min(d$LM)-0.15 , to=max(d$LM)+0.15 , length.out=30 )
Nseq <- seq( from=min(d$N)-0.15 , to=max(d$N)+0.15 , length.out=30 )
Zeroseq <- rep(0, 30)
Oneseq <- rep(1, 30)

# Counterfactual for Mass, holding Neocortex constant at 0
mu_counterM <- m5.7_post_matrix[,c("a", "bN", "bLM")] %*% rbind(Oneseq, Zeroseq, LMseq)
mu_mean_counterM <- apply(mu_counterM, 2, mean)
mu_PI_counterM <- apply(mu_counterM, 2, rethinking::PI)

# Counterfactual for Mass, holding Neocortex constant at 0
mu_counterN <- m5.7_post_matrix[,c("a", "bN", "bLM")] %*% rbind(Oneseq, Nseq, Zeroseq)
mu_mean_counterN <- apply(mu_counterN, 2, mean)
mu_PI_counterN <- apply(mu_counterN, 2, rethinking::PI)

# Plot the two counterfactual plots
par(mfcol=c(1,2))

plot(NULL, xlim=range(d$N), ylim=range(d$K), 
     main="Counterfactual plot at Body Mass=0",
     xlab="Neocortex % (std)",
     ylab="Kcal im Milk (std)")
lines(Nseq, mu_mean_counterN, lwd=2)
shade(mu_PI_counterN, Nseq)

plot(NULL, xlim=range(d$LM), ylim=range(d$K), 
     main="Counterfactual plot at N=0",
     xlab="Log Body Masss (std)",
     ylab="Kcal im Milk (std)")
lines(LMseq, mu_mean_counterM, lwd=2)
shade(mu_PI_counterM, LMseq)
```



# 5.3: Categorical Variables
Basically, this section deals with the syntax/ idiom to define categorical variables. No discussion of the math/ stats happening under the hood. 

```{r}
library(forcats)
library(dplyr)
data(milk)
#d <- na.omit(milk)
d$K <- scale(d$kcal.per.g)
d$M <- scale(log(d$mass))
d$clade_id <- as.integer( d$clade )
# forcats::fct_count(d$clade)
# d %>% group_by(clade) %>% summarize(., mean=mean(K), sd=sd(K), n())
par(mfcol=c(1,2))
boxplot(K ~ clade, data = d, 
        main="Milk energe by clade",
        names=c("Ape", "Old W", "New W", "Strep."))
abline(h=0, lty=2)

boxplot(M ~ clade, data = d,
        main="Body mass by clade",
        names=c("Ape", "Old W", "New W", "Strep."))
abline(h=0, lty=2)
```

```{r}
m5.9_A <- quap(
  alist(
    K ~ dnorm(mu, sigma),
    mu <- a[clade_id] + bM*M,
    a[clade_id] ~ dnorm(0, 0.9),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

m5.9_B <- quap(
  alist(
    K ~ dnorm(mu, sigma),
    mu <- a[clade_id],
    a[clade_id] ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

labels <- paste( "a[" , 1:4 , "]:" , levels(d$clade) , sep="" )

par(mfcol=c(2,1))
plot( precis( m5.9_B , depth=2 , pars="a" ) , labels=labels ,
      main="No conditioning for body mass",
    xlab="expected kcal (std)" )
plot( precis( m5.9_A , depth=2 , pars="a" ) , labels=labels ,
      main="Conditioning on body mass",
    xlab="expected kcal (std)" )
```



# Practise Questions

# Easy

# Medium

# Hard