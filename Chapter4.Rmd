---
title: "Chapter4"
output: html_document

---

```{r include=FALSE}
# Setting Knit options for the document
knitr::opts_chunk$set( message=FALSE, warning=FALSE)
```

# Notes from the Chapter

## 4.1: Why is it ok to use Gaussian distribution as a model?
We discusss reasons why modelling some variable or phonomenon as Gaussian is ok. 
- Ontological: As a category, Gaussian distributions are faily common in the wild. 
- Epistemological: If all we know is the average value of a variable, and a sense of the dispersion of the variable, then the Gaussian distribution is the most conservative distribution incorporating that information. 

## 4.2 
- We discusss the `~` notation used to map variables to statistical distributions/ models. 

## 4.3 A Gaussian model for height

- Our goal in this chapter is to fit a $Normal(\mu, \sigma)$ distribution to a given dataset, subject to a given prior, and to obtain the most likely values for $\mu$ and $\sigma$. 
- From the typical Bayesian updating we have 
$$ P(\theta | X) = \frac{P(X|\theta) P(\theta)}{P(X)}$$
However, here $\theta$ is a vector of two parameters $\mu$ and $\sigma$, giving us

$$ P(\mu, \sigma | X) = \frac{P(X|\mu, \sigma) P(\mu, \sigma)}{P(X)}$$
We're modelling the data as Gaussian, meaning $P(X|\mu, \sigma)$ = $Normal(X=x_0| \mu, \sigma)$
We're also silently asssuming that $\mu$ and $\sigma$ are independent, giving us $P(\mu, \sigma)$ = $P(\mu)*P(\sigma)$

### Generating the posterior -- the usual grid approximation method

```{r}
library(rethinking)
data <- data(Howell1)
d2 <- Howell1[Howell1$age >= 18,]

mu.list <- seq( from=150, to=160 , length.out=100 )
sigma.list <- seq( from=7 , to=9 , length.out=100 )
post <- expand.grid( mu=mu.list , sigma=sigma.list )

```

post$LL calculates the log likelihood of the data for every mu, sigma pair i.e log of P(X|mu, sigma)

```{r}
post$LL <- sapply( 1:nrow(post) , function(i) sum(
    dnorm( d2$height , post$mu[i] , post$sigma[i] , log=TRUE ) ) )
```

post$Prod is log of (Likelihood * prior ) log(likelihood) + log(prior)
```{r}
post$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) +
    dunif( post$sigma , 0 , 50 , TRUE )
```

So, now each row contains the log of LikelihoodxPrior against the specific mu-sigma pair in that row. Now, to convert log into probability, we need to exponentiate it. 
But, since the logged figures are large negative numbers floating point arithmetic errors will simply send exp(large negative value) to zero. To avoid this, instead of exp(LL), we calculate exp(LL - max(LL)), LL - max(LL) will return a small positive value. 

```{r}
scale_constant <- max(post$prod)
post$prod2 <- exp(post$prod - scale_constant)
```

All that is now left is to re-scale the probabilities (the denominator term P(X)). We may naturally ask, shouldn't we re-adjust the probabilities to offset the scaling factor? 
But we can be clever here. Notice that $\exp(x_i-y) = \frac{\exp(x_i)}{\exp(y)} = \frac{\exp(x_i)}{\exp(\rm{scale-constant})} = \exp(x_i)*\lambda$
Giving us $\frac{\exp(x_i - y)}{\sum_iexp(x_i -y)} = \frac{exp(x_i)*\lambda}{\sum_iexp(x_i)*\lambda} = \frac{exp(x_i)}{\sum_iexp(x_i)}$

In short, no we don't need to re-adjust to offset the scaling factor, leaving us with:

```{r}
post$prob <- post$prod2/sum(post$prod2)
image_xyz( post$mu , post$sigma , post$prob )
```

We notice that the values for `sigma` are much more dispersed, than the values for `mu`. We can plot the posterior for mu and sigma to confirm this. 

```{r}
plot(post$mu, post$prob)
plot(post$sigma, post$prob)
```

### Is the posterior Gaussian? 
 Well, the posterior for `mu` looks Gausssian, the posterior for `sigma` has a thickeer and longer right tail. 

ss
### 4.3.4 Sampling the posterior -- good old sample proportional to posterior probability
```{r}
sample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE ,
    prob=post$prod2 )
sample.mu <- post$mu[ sample.rows ]
sample.sigma <- post$sigma[ sample.rows ]
plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )
smoothScatter( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )
```

### 4.3.5 Generating and Sampling the posterior -- Quadratic approximation
- McLerath does not focus on 'what' the Quadratic approximation does, or even 'how' QAP does what it does. He dives straight into how to work with QAP. So, bummer. 
- So, my sense is QAP approximates the posterior as a multivariate normal distribution with two paramters mu and sigma. 
- And a multivariate normal distribution is fully defined by the vector of the parameter means and a covariance matrix b/w the parameters. So, effectlvey, the QAP extracts out the mean vector and covariance matrix for the posterior.
- Not sure what the justification for this approach is. Are there limitations to this approach? 

- Sampling from the posterior is equivalent to generating a multivariate normal distribution using the mean vector and covariance matrix. 

```{r}
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ]

flist <- alist(
    height ~ dnorm( mu , sigma ),
    mu ~ dnorm( 178 , 20 ),
    sigma ~ dunif( 0 , 50 )
)

start <- list(
    mu=mean(d2$height),
    sigma=sd(d2$height)
)

m4.1 <- quap(flist , data=d2 , start=start)
post_qap <- extract.samples( m4.1 , n=1e4 )
```

```{r}
plot(post_qap, col=col.alpha(rangi2,0.1))
plot(density(post_qap$sigma), main="QAP v/s Grid Sampling")
lines(density(sample.sigma),lwd=2,lty=2, col="blue")
# Looks like relative error in the tails is large. 
```


## 4.4 Linear Prediction
```{r}
 plot( d2$height ~ d2$weight )
```

Clearly, there's a strong positive relationship b/w weight and height. 

### Section 1: Setting up the linear prediction model

$$ 
height_i \sim Normal(\mu_i,\sigma) \\
\mu_i = \alpha + \beta(x_i- \bar{x}) \\
\alpha \sim Normal(178, 20) \\
\beta \sim Normal(0,10) \\
\sigma \sim Uniform(0,50) 
$$

#### Sanity check the priors

McLerath sanith checks the priors for $\beta$. We see that a Normal(0,10) priors results in a bunch of lines where height seems to decrease with average weight, something patently false as observed from our inspection. We revise our model to make $\beta$ positive always. 

$$\beta \sim LogNormal(0,1)$$

The log-normal distribution looks like this:

```{r}
x_seq <- seq(from=-10, to = 10, length.out = 1000)
plot(x_seq,dlnorm(x_seq, 0 , 2),type="l",lwd=2)
lines(x_seq,dlnorm(x_seq, 0 , 1.5 ),col="blue",lwd=2)
lines(x_seq,dlnorm(x_seq, 0 , 1 ),col="red",lwd=3)
lines(x_seq,dlnorm(x_seq, 0 , 0.5 ),col="brown",lwd=2)
lines(x_seq,dlnorm(x_seq, 1 , 1 ),col="yellow",lwd=2)
```

So basically, the log normal distro assigns 0 probability to negative values, and has a very sharp probability spike around the mean value, subject to the dispersion parameter. 

#### An attempted grid search to generate the posterior.
Grid search becomes unmanageably large real quick. Save this for later. 

```{r}
# library(rethinking)
# data(Howell1)
# d <- Howell1
# d2 <- d[ d$age >= 18 , ]
# define the average weight, x-bar
# xbar <- mean(d2$weight)
# sigma.list <- runif( 100 , 0 , 50 )
# alpha.list <- rnorm( 100 , 178 , 20 )
# beta.list <- rnorm( 100 , 0 , 10 )
# post <- expand.grid( alpha=alpha.list , beta=beta.list, xi_xbar = d2$height-xbar)
# post$mu_i <- post$alpha + post$beta*post$xi_xbar
# post_height <- expand.grid(mu_i = post$mu_i,sigma= sigma.list)
# post_height$LL <- sapply(1:nrow(post_height) ,
#                          function(i) sum(dnorm(d2$height, post_height$mu_i[i], post_height$sigma[i],log = TRUE )))

```

#### Using QUAP to fit the model and generate a posterior

```{r}
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ]
# define the average weight, x-bar
xbar <- mean(d2$weight)

# Fit the model
m4_3 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*(weight-xbar),
    a ~ dnorm(178,20),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ),
  data = d2
)
```

```{r}
precis(m4_3)
coef(m4_3)
round(vcov(m4_3),3)
```

We can now examine the posterior distribution we just generated. 

```{r}
post_m4_3 <- MASS::mvrnorm( n=1e4 , mu=coef(m4_3) , Sigma=vcov(m4_3) )
plot(density(post_m4_3[,"a"]))
```

Note that we are implicitly following the resampling approach to posterior probability for parameters. In other words, the relatively posterior probability of parameter values is encoded via the # of times that value appearss in our resampled vector. There is no separaate vector or function to generate posterior probaabilities.

##### What is the predicted averagee height over all weight levels? 

$$
E(height) = E(Normal(\mu_i,\sigma)) = E(\mu_i) = E(\alpha + \beta(x-\bar{x}))
$$
Since we have alpha and beta as independent (Check the covariance matrix for confirmation), we have
$$
E(\alpha + \beta(x-\bar{x}) = E(\alpha) + E(\beta)*E(x) - E(E(\bar{x})) = E(\alpha)
$$
Using the sample mean as the estimator, we get $E(\alpha)$ = `mean(post_m4_3[,"a"])`

```{r}
mean(post_m4_3[,"a"])
```

##### How is average height related to weight? 

What does this question even mean? Let's interpret it to mean the locus of the average height conditional on weight. 

$$
E(height | weight) = E(Normal(\mu_i,\sigma)| weight) \\
= E(\mu_i|weight) = E(\alpha + \beta(x-\bar{x})|x) \\
= E(\alpha|x) + E(\beta|x)*E(x|x) - E(\bar{x}|x) \\
= E(\alpha|x) + E(\beta|x)*(x - \bar{x})
$$

Now, how do we find things like $E(\alpha|x)$? Well, if you think about it, we have defined $\alpha$ and $\beta$ to be independent of x. So, $E(\alpha|x) = E(\alpha)$ 

The locus of conditional means then is 
$$E(\alpha) + E(\beta)*(x - \bar{x})$$
```{r}
e_alpha <- mean(post_m4_3[,"a"])
e_beta <- mean(post_m4_3[,"b"])
plot( height ~ weight , data=d2 , col=rangi2 )
curve( e_alpha + e_beta*(x - xbar) , add=TRUE )
```

##### What is the uncertainty in the relationship b/w weight and average height for that weight? 

So, we want $Variance(\mu_i|weight)$

```{r}
mu_at_weight <- function(weight, sample_matrix, avg_weight) {
                        sample_matrix[,"a"] + sample_matrix[,"b"]*(weight - avg_weight)
                  }

mu_at_50 <- mu_at_weight(50, post_m4_3, xbar) 
plot(density(mu_at_50))
```

```{r}
weight.seq <- seq( from=25 , to=70 , by=1 )
mu <- sapply( weight.seq , mu_at_weight, sample_matrix=post_m4_3, avg_weight=xbar )
  ## So each row in the mu matrix corresponds to a value of my calculated using the values in one sample from the posterior.
  ## Each column corresponds to one unique value from the sequence 25 to 70.
mu.mean <- apply( mu , 2 , mean ) ## We average over each column to obtain the average mu for a particular weight
mu.CI <- apply( mu , 2 , PI , prob=0.89 ) ## We also find the 89% CI for mu against each weight column
```

```{r}
# use type="n" to hide raw data
plot( height ~ weight , d2 , type="n" )

# Sample a few rows
plot_rows <- sample(1:nrow(mu),1000)

# Loop over sampled rows, and plot the mu values in the row
for (i in plot_rows){
  points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1))  
}

plot(density(mu_at_50))

```

So this graph suggests that each of these clumps of points is a normal distribution with mean centered at $\mu|weight$

Here's a small graph to show that the uncertainty in mu (average weight) is a function of distance from the mean weight (xbar).
30 and 60 are at the same distance from xbar ~ 15 kg. Their 89% CI has the same width. 

```{r}
mu_CI_width <- mu.CI[2,] - mu.CI[1,]
plot(weight.seq, mu_CI_width)
abline(h=mu_CI_width[which(weight.seq==60)])
abline(v=c(30,60))
```




##### Model height itself, rather than average height

We have $height_i \sim Normal(\mu_i,\sigma)$. In the previous sections we generated $\sigma$ using the $\alpha$ and $\beta$ values from the posterior. We will not use the same $\alpha$ and $\beta$ values along with $\sigma$ values to simulate height

```{r}
# Sampled posterior is in post_m4_3
# for each weight
#    do: 
#     mu_vector = a + b*(weight - avg weight)
#     sigma_vector = sigma
#     generate 1000 draws from rnorm(mean=mu_vector, sd=sigma_vector) 

weight_seq <- seq(from=25, to = 70, by = 1)
height_sim <- sapply(weight_seq, function(weight){
                      rnorm( n=1000, mean = post_m4_3[,"a"] + post_m4_3[,"b"]*(weight-xbar), sd=post_m4_3[,"sigma"])
})
height_mean <- apply(height_sim, 2, mean)
height_PI <- apply(height_sim, 2, rethinking::PI, prob=0.89)
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) )
points(weight_seq, height_mean,pch=2)
shade(height_PI, weight_seq)
shade(mu.CI, weight_seq, col=col.alpha(rangi2,0.5))
e_alpha <- mean(post_m4_3[,"a"])
e_beta <- mean(post_m4_3[,"b"])
curve( e_alpha + e_beta*(x - xbar) , add=TRUE )



```





## 4.5 Curves from lines: Polynomials and Basis functions

"But there's nothing special about straight lines" I mean, there kinda is -- constant slope. Which means that the relationship b/w the outcome and the predictor variable is constant across levels of the predictor variable. 

### Polynomical regresssions

Nothing fancy going on here, we just add higher powers of the predictor variable $x^2, x^3$ etc.
Makes it hard to interpret coefficients though. Also prone to overfitting. 

The basic recipe stays the same: 
1. Define a likelihood for the outcome variable. Typically the likelihood is some statistical distribution (eg Normal Distro with a mean and a variance parameter). 
2. Link the parameters of the liklihood function to the predictor variables using a linear functional form.
3. Apply priors for the components of the parameters.
4. Fit the posterior distribution using Quadiatic Approximation
5. Re-sample from the posterior to obtain values of various parameters, and sub-parameters along with their relative probabilities.
6. Use the re-sampled parameter values to predict and draw inferences. 

#### Fitting a polynomial model to the !Kung data

$$
height_i \sim Normal(\mu_i, sigma) \\
\mu_i = \alpha + \beta_1\hat{x_i} + \beta_2\hat{x_i}^2, \textrm{where} \space \hat{x_i} = x_i - \bar{x}\\
\alpha \sim Normal(178,20)  \\
\beta_1 \sim LogNormal(0,1) \\
\beta_2 \sim Normal(0,10) \\
\sigma \sim Uniform(0,50)
$$

```{r}
library(rethinking)
data(Howell1)
d2 <- Howell1

# define the average weight, x-bar
xbar <- mean(d2$weight)

# define de-meaned weight and it's square
d2$weight_std = d2$weight - xbar
d2$weight_std_2 = d2$weight_std^2

# Fit the model
m4_4 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b_1*weight_std + b_2*weight_std_2,
    a ~ dnorm(178,20),
    b_1 ~ dlnorm(0,1),
    b_2 ~ dnorm(0,10),
    sigma ~ dunif(0,50)
  ),
  data = d2
)

post_m4_4 <- MASS::mvrnorm( n=1e4 , mu=coef(m4_4) , Sigma=vcov(m4_4) )
```




### Spline regressions

In statistics, a spline is a smooth function built out of smaller, component functions.

Unlike polynomial regression, B-splines do not directly transform the predictor by squaring or cubing it. Instead they invent a series of entirely new, synthetic predictor variables. Each of these variables serves to gradually turn a specific parameter on and off within a specific range of the predictor variables. Each of these variables is called a basis function

```{r}
library(rethinking)
data(cherry_blossoms)
d <- cherry_blossoms
```



## Practice Questions

### Easy
#### 4E1. 
Q: In the model definition below, which line is the likelihood?
      yi ∼ Normal(μ, σ)
      μ ∼ Normal(0, 10) 
      σ ∼ Exponential(1)
      
Ans: Likelihoodn = P(data|parameters), y_i fits this description, and is hence the Likelihood.

#### 4E2. 
Q: In the model definition just above, how many parameters are in the posterior distribution?
Ans: 2 parameters, mu and sigma, both of which are first degree parameters without any sub-parameters

####4E3. 
Q: Using the model definition above, write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors.

Ans: Fill in the values in the following formula

$$
Pr(\mu,\sigma|h) = \frac{\Pi_iNormal(h_i,\mu,\sigma)*Normal(\mu|0,10)*Exp(\sigma|1)}{\int_\mu\int_\sigma\Pi_iNormal(h_i,\mu,\sigma)*Normal(\mu|0,10)*Exp(\sigma|1)d\mu d\sigma}
$$

#### 4E4. 
Q: In the model definition below, which line is the linear model? 
  yi ∼ Normal(μ, σ)
  μi =α+βxi
  α ∼ Normal(0, 10) 
  β ∼ Normal(0, 1) 
  σ ∼ Exponential(2)

Ans: A linear model relates a parameter as a linear function of predictor variables. The definition of $\mu_i$ fits the bill. 

#### 4E5. 
Q: In the model definition just above, how many parameters are in the posterior distribution?
Ans: 3 parameters, $\alpha, \beta, \sigma$



### Medium
#### 4M1. 
For the model definition below, simulate observed y values from the prior (not the posterior). 
  yi ∼ Normal(μ, σ)
  μ ∼ Normal(0, 10) 
  σ ∼ Exponential(1)

```{r}
length <- 1000 # 
mu_vector <- rnorm(length, mean = 0, sd = 10)
sigma_vector <- rexp(length, rate=1)
y_sim <- expand.grid(mu=mu_vector,sigma=sigma_vector)
y_sim$y <- rnorm(y_sim$mu,y_sim$sigma)
plot(density(y_sim$y))
```




#### 4M2. 
Translate the model just above into a quap formula.

Ans: 
`
q4m2_model <- quap(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- dnorm(0, 10),
    sigma ~ dexp(1)
  ),
  data = data
)
`

4M3. 
Translate the quap model formula below into a mathematical model definition.
flist <- alist(
    y ~ dnorm( mu , sigma ),
    mu <- a + b*x,
    a ~ dnorm( 0 , 10 ),
    b ~ dunif( 0 , 1 ),
    sigma ~ dexp( 1 )
)